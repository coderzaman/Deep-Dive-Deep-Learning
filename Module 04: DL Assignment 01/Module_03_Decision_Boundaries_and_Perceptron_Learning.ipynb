{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Module 03: Decision Boundaries and Perceptron Learning\n",
        "\n",
        "**Q1. What is a decision boundary, and how does it relate to classification tasks?**\n",
        "\n",
        "**‡¶â‡¶§‡ßç‡¶§‡¶∞:** ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï ‡¶∏‡ßÄ‡¶Æ‡¶æ‡¶®‡¶æ‚Äî‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡ßá‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü ‡¶è‡¶ü‡¶ø ‡ß®‡¶°‡¶ø (2D) ‡¶∏‡ßç‡¶™‡ßá‡¶∏‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶è‡¶¨‡¶Ç ‡ß©‡¶°‡¶ø (3D) ‡¶∏‡ßç‡¶™‡ßá‡¶∏‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡¶§‡¶≤ (hyperplane)‡•§ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶ü‡¶æ‡¶∏‡ßç‡¶ï‡ßá ‡¶è‡¶∞ ‡¶Æ‡ßÇ‡¶≤ ‡¶ï‡¶æ‡¶ú ‡¶π‡¶≤‡ßã ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡ßá‡¶∞ ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá (‡¶Ø‡ßá‡¶Æ‡¶®: ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡ß¶ ‡¶è‡¶¨‡¶Ç ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ ‡ßß) ‡¶è‡¶ï‡ßá ‡¶Ö‡¶™‡¶∞‡ßá‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ü‡¶≤‡¶æ‡¶¶‡¶æ ‡¶ï‡¶∞‡¶æ‡•§ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶®‡¶ø‡¶Ç‡ßü‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶™‡ßç‡¶Ø‡¶æ‡¶ü‡¶æ‡¶∞‡ßç‡¶® ‡¶∂‡¶ø‡¶ñ‡ßá ‡¶è‡¶á ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá, ‡¶Ø‡¶æ‡¶§‡ßá ‡¶®‡¶§‡ßÅ‡¶® ‡¶ï‡ßã‡¶®‡ßã ‡¶°‡ßá‡¶ü‡¶æ ‡¶Ü‡¶∏‡¶≤‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡¶ü‡¶ø ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® ‡¶¶‡ßá‡¶ñ‡ßá ‡¶¨‡¶≤‡ßá ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶∏‡ßá‡¶ü‡¶ø ‡¶ï‡ßã‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá ‡¶™‡ßú‡¶¨‡ßá‡•§\n",
        "\n",
        "**Q2. How does the bias term in a neuron affect the position of the decision boundary?**\n",
        "\n",
        "**‡¶â‡¶§‡ßç‡¶§‡¶∞:** ‡¶®‡¶ø‡¶â‡¶∞‡¶®‡ßá ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ (Bias) ‡¶ü‡¶æ‡¶∞‡ßç‡¶Æ‡¶ü‡¶ø ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶Ö‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡¶® (Position) ‡¶∂‡¶ø‡¶´‡¶ü ‡¶ï‡¶∞‡¶§‡ßá ‡¶¨‡¶æ ‡¶∏‡¶∞‡¶æ‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá‡•§ ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø ‡¶¨‡¶æ‡¶ß‡ßç‡¶Ø ‡¶π‡ßü‡ßá ‡¶∏‡¶¨‡¶∏‡¶Æ‡ßü ‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡ßá‡¶∞ ‡¶Æ‡ßÇ‡¶≤‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶¨‡¶æ ‡¶Ö‡¶∞‡¶ø‡¶ú‡¶ø‡¶® (0,0) ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ø‡ßá‡¶§‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶´‡¶≤‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡¶ü‡¶ø ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø‡¶ï‡ßá ‡¶°‡¶æ‡¶®‡ßá-‡¶¨‡¶æ‡¶Æ‡ßá ‡¶¨‡¶æ ‡¶ì‡¶™‡¶∞‡ßá-‡¶®‡¶ø‡¶ö‡ßá ‡¶∏‡¶∞‡¶ø‡ßü‡ßá ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶ï‡ßÉ‡¶§ ‡¶¨‡¶ø‡¶∏‡ßç‡¶§‡¶æ‡¶∞‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶®‡¶ø‡¶ú‡ßá‡¶ï‡ßá ‡¶Ü‡¶∞‡¶ì ‡¶®‡¶ø‡¶ñ‡ßÅ‡¶Å‡¶§‡¶≠‡¶æ‡¶¨‡ßá ‡¶Æ‡¶æ‡¶®‡¶ø‡ßü‡ßá ‡¶®‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\n",
        "\n",
        "**Q3. Explain with an example how changing weights affects the orientation of the decision boundary.**\n",
        "\n",
        "**‡¶â‡¶§‡ßç‡¶§‡¶∞:** ‡¶ì‡ßü‡ßá‡¶ü (Weights) ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶´‡¶≤‡ßá ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø‡¶∞ ‡¶¢‡¶æ‡¶≤ (Slope) ‡¶¨‡¶æ ‡¶¶‡¶ø‡¶ï (Orientation) ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§ ‡¶π‡ßü‡•§\n",
        "\n",
        "‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£‡¶∏‡ßç‡¶¨‡¶∞‡ßÇ‡¶™, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡¶æ‡¶∞‡¶∏‡ßá‡¶™‡¶ü‡ßç‡¶∞‡¶®‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£ ‡¶ß‡¶∞‡ßÅ‡¶®:  $w_1x_1 + w_2x_2 + b = 0$‡•§\n",
        "\n",
        "‡¶ó‡ßç‡¶∞‡¶æ‡¶´‡ßá ‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡ßá‡¶∂ ‡¶ï‡¶∞‡ßá‡•§ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá  $w_1$ ‡¶è‡¶¨‡¶Ç $w_2$  ‡¶π‡¶≤‡ßã ‡¶∞‡ßá‡¶ñ‡¶æ‡¶ü‡¶ø‡¶∞ ‡¶¢‡¶æ‡¶≤‡•§  ‡¶Ü‡¶™‡¶®‡¶ø ‡¶Ø‡¶¶‡¶ø $w_1$ ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡¶æ‡ßú‡¶ø‡ßü‡ßá ‡¶¶‡ßá‡¶®, ‡¶§‡¶¨‡ßá ‡¶∞‡ßá‡¶ñ‡¶æ‡¶ü‡¶ø $x_1$ ‡¶Ö‡¶ï‡ßç‡¶∑‡ßá‡¶∞ ‡¶¶‡¶ø‡¶ï‡ßá ‡¶Ü‡¶∞‡¶ì ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡ßá‡¶≤‡ßá ‡¶™‡ßú‡¶¨‡ßá‡•§ ‡¶ì‡ßü‡ßá‡¶ü ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶Æ‡ßÇ‡¶≤‡¶§ ‡¶è‡¶á ‡¶∞‡ßá‡¶ñ‡¶æ‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶è‡¶ï‡¶ü‡ßÅ ‡¶ï‡¶∞‡ßá ‡¶ò‡ßã‡¶∞‡ßá (rotate), ‡¶Ø‡¶§‡¶ï‡ßç‡¶∑‡¶£ ‡¶®‡¶æ ‡¶è‡¶ü‡¶ø ‡¶∏‡¶†‡¶ø‡¶ï ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\n",
        "\n",
        "**Q4. Why is it important to visualize the decision boundary when training a model?**\n",
        "\n",
        "**‡¶â‡¶§‡ßç‡¶§‡¶∞:** ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ü‡ßç‡¶∞‡ßá‡¶á‡¶®‡¶ø‡¶Ç‡ßü‡ßá‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø ‡¶≠‡¶ø‡¶ú‡ßç‡¶Ø‡ßÅ‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡¶æ ‡¶¨‡ßá‡¶∂ ‡¶ï‡ßü‡ßá‡¶ï‡¶ü‡¶ø ‡¶ï‡¶æ‡¶∞‡¶£‡ßá ‡¶Ö‡¶§‡ßç‡¶Ø‡¶®‡ßç‡¶§ ‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø:\n",
        "‡ßß. ‡¶Æ‡¶°‡ßá‡¶≤‡¶ü‡¶ø ‡¶Ü‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶´‡¶ø‡¶ü (Underfit) ‡¶®‡¶æ‡¶ï‡¶ø ‡¶ì‡¶≠‡¶æ‡¶∞‡¶´‡¶ø‡¶ü (Overfit) ‡¶ï‡¶∞‡¶õ‡ßá ‡¶§‡¶æ ‡¶∏‡¶π‡¶ú‡ßá‡¶á ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡ßü‡•§\n",
        "\n",
        "‡ß®. ‡¶°‡ßá‡¶ü‡¶æ‡¶∏‡ßá‡¶ü‡¶ü‡¶ø ‡¶≤‡¶ø‡¶®‡¶ø‡ßü‡¶æ‡¶∞‡¶≤‡¶ø ‡¶∏‡ßá‡¶™‡¶æ‡¶∞‡ßá‡¶¨‡¶≤ (Linearly Separable) ‡¶ï‡¶ø‡¶®‡¶æ ‡¶§‡¶æ ‡¶¨‡ßã‡¶ù‡¶æ ‡¶Ø‡¶æ‡ßü (‡¶Ø‡ßá‡¶Æ‡¶® XOR ‡¶™‡ßç‡¶∞‡¶¨‡¶≤‡ßá‡¶Æ ‡¶ï‡¶ø ‡¶®‡¶æ)‡•§\n",
        "\n",
        "‡ß©. ‡¶Æ‡¶°‡ßá‡¶≤‡¶ü‡¶ø ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶ï‡¶§‡¶ü‡¶æ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡ßÄ‡¶Æ‡¶æ‡¶®‡¶æ ‡¶ü‡¶æ‡¶®‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶õ‡ßá, ‡¶∏‡ßá ‡¶∏‡¶Æ‡ßç‡¶™‡¶∞‡ßç‡¶ï‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü‡•§\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Math: Weight Update (Perceptron Learning Rule)\n",
        "\n",
        "**‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶Ü‡¶õ‡ßá (Given):**\n",
        "\n",
        "* Initial weights, $w = [0.2, -0.1]$\n",
        "* Bias, $b = 0.1\n",
        "* $Learning rate, $\\eta = 0.1$\n",
        "* Input, $x = [1, 1]$\n",
        "* Target, $t = 1$\n",
        "* Output, $y = 0$\n",
        "\n",
        "**‡¶ß‡¶æ‡¶™ ‡ßß: Error ‡¶¨‡¶æ ‡¶≠‡ßÅ‡¶≤‡ßá‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ**\n",
        "\n",
        "* Error = $(t - y)$\n",
        "\n",
        "* Error = $(1 - 0) = 1$\n",
        "\n",
        "**‡¶ß‡¶æ‡¶™ ‡ß®: ‡¶®‡¶§‡ßÅ‡¶® ‡¶ì‡ßü‡ßá‡¶ü ($w_{new}$) ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ï‡ßÅ‡¶≤‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ**\n",
        "‡¶∏‡ßÇ‡¶§‡ßç‡¶∞: $w_{new} = w + \\eta \\cdot (t - y) \\cdot x$\n",
        "\n",
        "$w_{new} = [0.2, -0.1] + 0.1 \\cdot 1 \\cdot [1, 1]$\n",
        "\n",
        "$w_{new} = [0.2, -0.1] + [0.1, 0.1]$\n",
        "\n",
        "$w_{new} = [0.2 + 0.1, -0.1 + 0.1]$\n",
        "\n",
        "$w_{new} = [0.3, 0.0]$\n",
        "\n",
        "****\n",
        "\n",
        "**‡¶ß‡¶æ‡¶™ ‡ß©: ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏  ($b_{new}$)  ‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ï‡ßÅ‡¶≤‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ**\n",
        "‡¶∏‡ßÇ‡¶§‡ßç‡¶∞:\n",
        "$b_{new} = b + \\eta \\cdot (t - y)$\n",
        "\n",
        "$b_{new} = 0.1 + 0.1 \\cdot 1$\n",
        "\n",
        "$b_{new} = 0.1 + 0.1$\n",
        "\n",
        "$b_{new} = 0.2$\n",
        "\n",
        "****\n",
        "\n",
        "**‡¶ö‡ßÇ‡ßú‡¶æ‡¶®‡ßç‡¶§ ‡¶â‡¶§‡ßç‡¶§‡¶∞:** ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü‡ßá‡¶∞ ‡¶™‡¶∞ ‡¶®‡¶§‡ßÅ‡¶® ‡¶ì‡ßü‡ßá‡¶ü ‡¶π‡¶¨‡ßá $w = [0.3, 0.0]$ ‡¶è‡¶¨‡¶Ç ‡¶®‡¶§‡ßÅ‡¶® ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ ‡¶π‡¶¨‡ßá $b = 0.2$‡•§\n",
        "\n",
        "\n",
        "‡¶è‡¶á ‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®‡ßá‡¶∞ ‡¶ï‡ßã‡¶®‡ßã ‡¶ß‡¶æ‡¶™‡ßá ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶ï‡ßá ‡¶ú‡¶æ‡¶®‡¶æ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡¶®‡•§"
      ],
      "metadata": {
        "id": "NrEd7PbukK7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ó‡¶ø‡¶ü‡¶π‡¶æ‡¶¨ ‡¶®‡ßã‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Æ‡¶°‡¶ø‡¶â‡¶≤ ‡ß©-‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶ï‡¶ü‡¶ø‡¶∏ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ **English** ‡¶≠‡¶æ‡¶∞‡ßç‡¶∏‡¶® ‡¶®‡¶ø‡¶ö‡ßá ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞‡¶≠‡¶æ‡¶¨‡ßá ‡¶ó‡ßÅ‡¶õ‡¶ø‡ßü‡ßá ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶π‡¶≤‡ßã:\n",
        "\n",
        "### üìù Module 03: Decision Boundaries and Perceptron Learning\n",
        "\n",
        "**Q1. What is a decision boundary, and how does it relate to classification tasks?**\n",
        "\n",
        "**Answer:** A decision boundary is a mathematical line or surface‚Äîspecifically, a straight line in a 2D feature space and a flat plane (hyperplane) in a 3D space. In classification tasks, its primary purpose is to perfectly separate different classes of data (e.g., Class 0 and Class 1) from each other. During training, the model learns patterns from the data to construct this boundary, so that when new, unseen data arrives, the model can predict its class based on which side of the boundary it falls on.\n",
        "\n",
        "**Q2. How does the bias term in a neuron affect the position of the decision boundary?**\n",
        "\n",
        "**Answer:** The bias term in a neuron allows the decision boundary to shift its position. Without a bias, the decision boundary would be mathematically forced to always pass exactly through the origin (0,0) of the graph. By adding a bias, the model gains the flexibility to shift the boundary left, right, up, or down, allowing it to perfectly align with the actual distribution of the data.\n",
        "\n",
        "**Q3. Explain with an example how changing weights affects the orientation of the decision boundary.**\n",
        "\n",
        "**Answer:** Changing the weights fundamentally alters the slope, angle, or orientation of the decision boundary.\n",
        "\n",
        "For example, consider the perceptron equation: $w_1x_1 + w_2x_2 + b = 0$. On a graph, this represents a straight line. Here, the weights $w_1$ and $w_2$ determine the slope of that line. If you significantly increase the value of $w_1$, the line will tilt more toward the $x_1$ axis. During the weight update process, this line essentially rotates little by little until it finds the correct angle to classify the data correctly.\n",
        "\n",
        "**Q4. Why is it important to visualize the decision boundary when training a model?**\n",
        "\n",
        "**Answer:** Visualizing the decision boundary during model training is crucial for several reasons:\n",
        "\n",
        "1. It makes it easy to intuitively see if the model is underfitting or overfitting the training data.\n",
        "2. It helps determine if the dataset is linearly separable or if it requires non-linear boundaries (like the XOR problem).\n",
        "3. It provides a clear, visual understanding of how well the model is separating different clusters of data and where it might be making errors.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Math: Weight Update (Perceptron Learning Rule)\n",
        "\n",
        "**Given:**\n",
        "\n",
        "* Initial weights, $w = [0.2, -0.1]$\n",
        "* Bias, $b = 0.1$\n",
        "* Learning rate, $\\eta = 0.1$\n",
        "* Input,  $x = [1, 1]$\n",
        "* Target, $t = 1$\n",
        "* Output, $y = 0$\n",
        "\n",
        "**Step 1: Calculate the Error**\n",
        "Error = $(t - y)$\n",
        "Error = $(1 - 0) = 1$\n",
        "\n",
        "**Step 2: Calculate the new weights ($w_{new}$)**\n",
        "Formula: $w_{new} = w + \\eta \\cdot (t - y) \\cdot x$\n",
        "\n",
        "$w_{new} = [0.2, -0.1] + 0.1 \\cdot 1 \\cdot [1, 1]$\n",
        "$w_{new} = [0.2, -0.1] + [0.1, 0.1]$\n",
        "\n",
        "$w_{new} = [0.2, -0.1] + [0.1, 0.1]$\n",
        "\n",
        "$w_{new} = [0.2, -0.1] + [0.1, 0.1]$\n",
        "****\n",
        "\n",
        "**Step 3: Calculate the new bias ($b_{new}$)**\n",
        "\n",
        "Formula:  $b_{new} = b + \\eta \\cdot (t - y)$\n",
        "$b_{new} = 0.1 + 0.1 \\cdot 1$\n",
        "\n",
        "$b_{new} = 0.1 + 0.1$\n",
        "\n",
        "$b_{new} = 0.2$\n",
        "****\n",
        "\n",
        "Final Answer: After one update, the new weights will be $w = [0.3, 0.0]$ and the new bias will be $b = 0.2$."
      ],
      "metadata": {
        "id": "Q779aqaWlZOh"
      }
    }
  ]
}
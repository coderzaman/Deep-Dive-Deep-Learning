{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **üìù Practice Problem 01: Perceptron & Activation Functions**\n",
        "\n",
        "### **1. Bias**\n",
        "\n",
        "**Q: What is a bias in a neural network?**\n",
        "\n",
        "* **English:** A bias is an extra constant parameter added to the weighted sum of inputs before passing it to the activation function. It acts similarly to the intercept ($c$)  in a linear equation ($y = mx + c$).\n",
        "\n",
        "* **Bangla:** ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶§‡¶ø‡¶∞‡¶ø‡¶ï‡ßç‡¶§ ‡¶ï‡¶®‡¶∏‡ßç‡¶ü‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶ü ‡¶¨‡¶æ ‡¶ß‡ßç‡¶∞‡ßÅ‡¶¨‡¶ï, ‡¶Ø‡¶æ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶è‡¶¨‡¶Ç ‡¶ì‡ßü‡ßá‡¶ü‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶£‡¶´‡¶≤‡ßá‡¶∞ ‡¶Ø‡ßã‡¶ó‡¶´‡¶≤‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§ ‡¶∏‡¶∞‡¶≤‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßÄ‡¶ï‡¶∞‡¶£‡ßá ($y = mx + c$) $c$ ‡¶Ø‡ßá‡¶Æ‡¶® ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá, ‡¶®‡¶ø‡¶â‡¶∞‡¶æ‡¶≤ ‡¶®‡ßá‡¶ü‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶ï‡ßá ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏‡¶ì ‡¶†‡¶ø‡¶ï ‡¶è‡¶ï‡¶á ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá‡•§\n",
        "\n",
        "**Q: Why do we add bias to a perceptron?**\n",
        "\n",
        "* **English:** Bias allows the activation function to shift to the left or right. Without a bias, the model's boundary would always have to pass through the origin$(0,0)$, which severely limits its flexibility to fit real-world data.\n",
        "\n",
        "* **Bangla:** ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡¶ø‡¶≠‡ßá‡¶∂‡¶® ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡¶ï‡ßá ‡¶°‡¶æ‡¶®‡ßá ‡¶¨‡¶æ ‡¶¨‡¶æ‡¶Æ‡ßá ‡¶∏‡¶∞‡¶æ‡¶§‡ßá ‡¶∏‡¶æ‡¶π‡¶æ‡¶Ø‡ßç‡¶Ø ‡¶ï‡¶∞‡ßá‡•§ ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶°‡¶ø‡¶∏‡¶ø‡¶∂‡¶® ‡¶¨‡¶æ‡¶â‡¶®‡ßç‡¶°‡¶æ‡¶∞‡¶ø ‡¶∏‡¶¨‡¶∏‡¶Æ‡ßü ‡¶Æ‡ßÇ‡¶≤‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ$(0,0)$  ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ø‡ßá‡¶§‡•§ ‡¶°‡ßá‡¶ü‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá ‡¶Ü‡¶∞‡¶ì ‡¶≠‡¶æ‡¶≤‡ßã‡¶≠‡¶æ‡¶¨‡ßá ‡¶Æ‡¶æ‡¶®‡¶ø‡ßü‡ßá ‡¶®‡ßá‡¶ì‡ßü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶æ‡ßü‡¶æ‡¶∏ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§\n",
        "\n",
        "### **2. Weights**\n",
        "\n",
        "**Q: What are weights in a perceptron?**\n",
        "\n",
        "* **English:** Weights are learnable parameters that represent the strength or importance of each input signal. They are analogous to the slope ($m$) in a linear equation.\n",
        "\n",
        "* **Bangla:** ‡¶ì‡ßü‡ßá‡¶ü ‡¶π‡¶≤‡ßã ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶∂‡¶ø‡¶ñ‡¶®‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞, ‡¶Ø‡¶æ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶∏‡¶ø‡¶ó‡¶®‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶∞ ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨ ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶ï‡¶∞‡ßá‡•§ ‡¶ï‡ßã‡¶® ‡¶á‡¶®‡¶™‡ßÅ‡¶ü‡¶ü‡¶ø ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶§‡¶ü‡¶æ ‡¶ú‡¶∞‡ßÅ‡¶∞‡¶ø, ‡¶§‡¶æ ‡¶è‡¶á ‡¶ì‡ßü‡ßá‡¶ü‡ßá‡¶∞ ‡¶ì‡¶™‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞ ‡¶ï‡¶∞‡ßá‡•§\n",
        "\n",
        "**Q: How do weights affect the output of a perceptron?**\n",
        "\n",
        "* **English:** A higher absolute weight means the corresponding input has a greater impact on the final output. If a weight is 0, that specific input is completely ignored by the perceptron.\n",
        "* **Bangla:** ‡¶ï‡ßã‡¶®‡ßã ‡¶á‡¶®‡¶™‡ßÅ‡¶ü‡ßá‡¶∞ ‡¶ì‡ßü‡ßá‡¶ü ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶ì‡ßü‡¶æ‡¶∞ ‡¶Ö‡¶∞‡ßç‡¶• ‡¶π‡¶≤‡ßã ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶§‡ßà‡¶∞‡¶ø‡¶§‡ßá ‡¶∏‡ßá‡¶á ‡¶á‡¶®‡¶™‡ßÅ‡¶ü‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨ ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø‡•§ ‡¶ì‡ßü‡ßá‡¶ü ‡¶Ø‡¶¶‡¶ø 0 ‡¶π‡ßü, ‡¶§‡¶¨‡ßá ‡¶™‡¶æ‡¶∞‡¶∏‡ßá‡¶™‡¶ü‡ßç‡¶∞‡¶® ‡¶∏‡ßá‡¶á ‡¶á‡¶®‡¶™‡ßÅ‡¶ü‡¶ü‡¶ø‡¶ï‡ßá ‡¶™‡ßÅ‡¶∞‡ßã‡¶™‡ßÅ‡¶∞‡¶ø ‡¶¨‡¶æ‡¶§‡¶ø‡¶≤ ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡ßü‡•§\n",
        "\n",
        "### **3. Perceptron Output Calculation**\n",
        "\n",
        "**Q: A perceptron has inputs $x_1=2$, $x_2=3$, weights $w_1=0.5$, $w_2=1$, and bias $b=1$. Calculate the weighted sum (before activation).**\n",
        "\n",
        "\n",
        "* **Solution / ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®:**\n",
        "The formula for the weighted sum ($z$) is:\n",
        "\n",
        "$$z = (x_1 \\times w_1) + (x_2 \\times w_2) + b$$\n",
        "\n",
        "Now, plug in the given values:\n",
        "$$z = (2 \\times 0.5) + (3 \\times 1) + 1$$\n",
        "$$z = 1 + 3 + 1$$\n",
        "**Answer:** The weighted sum (before activation) is **5**.\n",
        "$$z = 5$$\n",
        "\n",
        "\n",
        "### **4. Step Function**\n",
        "\n",
        "**Q: What is the step function and how does it work?**\n",
        "\n",
        "* **English:** The Step function (or Heaviside step function) is a basic threshold-based activation function. It works by checking the weighted sum ($z$): if $z$ is greater than or equal to zero, it outputs $1$; otherwise, it outputs $0$ (or $-1$). It is primarily used for binary classification.\n",
        "\n",
        "* **Bangla:** ‡¶∏‡ßç‡¶ü‡ßá‡¶™ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶π‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶•‡ßç‡¶∞‡ßá‡¶∂‡ßã‡¶≤‡ßç‡¶°-‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡¶ï ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡¶ø‡¶≠‡ßá‡¶∂‡¶® ‡¶´‡¶æ‡¶Ç‡¶∂‡¶®‡•§ ‡¶è‡¶ü‡¶ø ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡¶ñ‡ßá ‡¶Ø‡ßá ‡¶Æ‡ßã‡¶ü ‡¶Ø‡ßã‡¶ó‡¶´‡¶≤ ($z$) ‡¶∂‡ßÇ‡¶®‡ßç‡¶Ø ‡¶¨‡¶æ ‡¶§‡¶æ‡¶∞ ‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßú ‡¶ï‡¶ø‡¶®‡¶æ‡•§ ‡¶¨‡ßú ‡¶π‡¶≤‡ßá ‡¶è‡¶ü‡¶ø $1$ ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶¶‡ßá‡ßü, ‡¶Ü‡¶∞ ‡¶õ‡ßã‡¶ü ‡¶π‡¶≤‡ßá $0$ (‡¶¨‡¶æ $-1$) ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶¶‡ßá‡ßü‡•§ ‡¶è‡¶ü‡¶ø ‡¶Æ‡ßÇ‡¶≤‡¶§ ‡¶¨‡¶æ‡¶á‡¶®‡¶æ‡¶∞‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ø‡¶´‡¶ø‡¶ï‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ï‡¶æ‡¶ú‡ßá ‡¶≤‡¶æ‡¶ó‡ßá‡•§\n",
        "\n",
        "### **5. Activation Function**\n",
        "\n",
        "**Q: Why do we need an activation function in a perceptron?**\n",
        "\n",
        "**English:** We need activation functions to introduce non-linearity into the network. Without it, a neural network (no matter how many layers it has) would just act like a simple linear regression model and fail to solve complex, non-linear real-world problems.\n",
        "\n",
        "**Bangla:** ‡¶Æ‡¶°‡ßá‡¶≤‡ßá **‡¶®‡¶®-‡¶≤‡¶ø‡¶®‡¶ø‡ßü‡¶æ‡¶∞‡¶ø‡¶ü‡¶ø (Non-linearity)** ‡¶¨‡¶æ ‡¶ú‡¶ü‡¶ø‡¶≤‡¶§‡¶æ ‡¶Ø‡ßÅ‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶ï‡ßç‡¶ü‡¶ø‡¶≠‡ßá‡¶∂‡¶® ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§ ‡¶è‡¶ü‡¶ø ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶®‡¶ø‡¶â‡¶∞‡¶æ‡¶≤ ‡¶®‡ßá‡¶ü‡¶ì‡ßü‡¶æ‡¶∞‡ßç‡¶ï‡ßá ‡¶Ø‡¶§‡¶ó‡ßÅ‡¶≤‡ßã ‡¶≤‡ßá‡ßü‡¶æ‡¶∞‡¶á ‡¶•‡¶æ‡¶ï‡ßÅ‡¶ï ‡¶®‡¶æ ‡¶ï‡ßá‡¶®, ‡¶è‡¶ü‡¶ø ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶≤‡¶ø‡¶®‡¶ø‡ßü‡¶æ‡¶∞ ‡¶∞‡¶ø‡¶ó‡ßç‡¶∞‡ßá‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã‡¶á ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶§ ‡¶è‡¶¨‡¶Ç ‡¶ú‡¶ü‡¶ø‡¶≤ ‡¶°‡ßá‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶∂‡¶ø‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶§ ‡¶®‡¶æ‡•§\n",
        "\n",
        "**Q: Name three common activation functions.**\n",
        "\n",
        "* **English / Bangla:** 1. **Sigmoid** (‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü  0 ‡¶•‡ßá‡¶ï‡ßá 1 ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∞‡¶æ‡¶ñ‡ßá)‡•§\n",
        "2. **ReLU** (Rectified Linear Unit - ‡¶®‡ßá‡¶ó‡ßá‡¶ü‡¶ø‡¶≠ ‡¶Æ‡¶æ‡¶®‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá 0 ‡¶ï‡¶∞‡ßá ‡¶¶‡ßá‡ßü ‡¶è‡¶¨‡¶Ç ‡¶™‡¶ú‡¶ø‡¶ü‡¶ø‡¶≠ ‡¶Æ‡¶æ‡¶® ‡¶†‡¶ø‡¶ï ‡¶∞‡¶æ‡¶ñ‡ßá)‡•§\n",
        "3. **Tanh** (Hyperbolic Tangent - ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü  -1 ‡¶•‡ßá‡¶ï‡ßá 1 ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶∞‡¶æ‡¶ñ‡ßá)‡•§"
      ],
      "metadata": {
        "id": "YKS0ffFnwODa"
      }
    }
  ]
}
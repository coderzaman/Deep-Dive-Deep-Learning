{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ“ Text Cell 1: Basics & Learning\n",
        "\n",
        "**Q1. In one sentence, what problem does a perceptron solve?**\n",
        "\n",
        "It solves strictly linear binary classification problems by drawing a single straight line to separate data into two classes.\n",
        "\n",
        "**Q2. What are the three main components of a perceptron?**\n",
        "\n",
        "1. Inputs and Weights, 2. Summation and Bias, 3. Activation Function.\n",
        "\n",
        "**Q3. Is perceptron a classifier or a regressor? Explain briefly.**\n",
        "\n",
        "It is a **classifier**. It uses a threshold (like a step function) to output discrete classes (e.g., $0$ or $1$) rather than a continuous numerical value like a regressor.\n",
        "\n",
        "**Q4. Why do we need to update weights in a perceptron?**\n",
        "\n",
        "To minimize the error. Updating weights adjusts the decision boundary until it correctly separates the classes.\n",
        "\n",
        "**Q5. What happens if we never update the weights during training?**\n",
        "\n",
        "The model will never \"learn.\" It will continue making random or incorrect predictions forever based on its initial random weights.\n",
        "\n",
        "**Q6. Which two quantities decide how much the weight changes?**\n",
        "\n",
        "The **Learning Rate ($\\eta$)** and the **Error ($y - \\hat{y}$)**. (The input value  is also multiplied with them).\n",
        "\n",
        "**Q7. What does the term  ($y - \\hat{y}$) represent in perceptron learning?**\n",
        "\n",
        "It represents the **Error**â€”the difference between the actual true label ($y$) and the model's predicted label  ($\\hat{y}$).\n",
        "\n",
        "**Q8. If the prediction is correct, will weights change? Why?**\n",
        "\n",
        "**No.** If the prediction is correct, the error ($y - \\hat{y}$)  becomes 0. Since weight update depends on multiplying by the error, the change () will be zero ($\\Delta w$).\n",
        "\n",
        "**Q9. What role does the learning rate  ($\\eta$)  play intuitively?**\n",
        "\n",
        "It controls the \"step size\" or speed of learning. A high rate means big weight changes (fast but might overshoot), and a low rate means small changes (slow but steady).\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“ Text Cell 2: Logic Gates, Geometry & Bias\n",
        "\n",
        "**Q10. Write the AND gate truth table.**\n",
        "\n",
        "* 0 AND 0 = 0\n",
        "* 0 AND 1 = 0\n",
        "* 1 AND 0 = 0\n",
        "* 1 AND 1 = 1\n",
        "\n",
        "**Q11. Why is AND gate linearly separable?**\n",
        "\n",
        "Because you can draw a single straight line on a 2D graph that perfectly separates the one positive output (1,1) from the three negative outputs (0,0; 0,1; 1,0).\n",
        "\n",
        "**Q12. Draw a rough decision boundary that separates AND gate outputs.**\n",
        "\n",
        "**Q13. Why do we add a bias term in the perceptron?**\n",
        "To allow the decision boundary to shift away from the origin  $(0,0)$, giving the model flexibility to fit real-world data properly.\n",
        "\n",
        "**Q14. Why is bias often added after summation?**\n",
        "Bias is mathematically a constant added to the total weighted sum ($z = \\sum wx + b$) right before it hits the activation function, determining the threshold needed to \"fire\" the neuron.\n",
        "\n",
        "**Q15. Why do we initialize weights with small random values?**\n",
        "To break symmetry. If all weights were zero, every input would have the exact same effect, and the model would struggle to learn the distinct importance of different features.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“ Text Cell 3: Code, Math & XOR Problem\n",
        "\n",
        "**Q16. What is the purpose of `np.dot(X, w)` in perceptron code?**\n",
        "\n",
        "It efficiently calculates the **weighted sum** by performing a dot product (matrix multiplication) between the input matrix `X` and the weight vector `w`.\n",
        "\n",
        "**Q17. Why is matrix transpose used in weight updating?**\n",
        "\n",
        "In code, a transpose is often needed to align the dimensions (rows and columns) of the input matrix and the error vector so that they can be multiplied mathematically.\n",
        "\n",
        "**Q18. What will happen if we remove the activation function?**\n",
        "\n",
        "The perceptron will just output the raw weighted sum, effectively becoming a simple **Linear Regression** model. It would lose its ability to classify strictly into 0 or 1.\n",
        "\n",
        "**Q19. Why can OR gate be solved using a single perceptron?**\n",
        "Just like AND, the OR gate's outputs (three 1s and one 0) can be perfectly separated by a single straight line on a graph.\n",
        "\n",
        "**Q20. What is common between AND and OR gates from a geometry view?**\n",
        "Both are **linearly separable**.\n",
        "\n",
        "**Q21. Does XOR need more features or more layers? Why?**\n",
        "It needs **more layers** (Hidden layers). A single layer can only draw one straight line, but XOR requires combining multiple lines to box in the diagonal data points.\n",
        "\n",
        "**Q22. What does linearly separable mean?**\n",
        "It means that data points belonging to different classes can be perfectly divided by a single straight line (in 2D) or a flat plane (in 3D).\n",
        "\n",
        "**Q23. Why canâ€™t a single straight line separate XOR data?**\n",
        "\n",
        "Because the identical classes are located on opposite diagonal corners of the graph. No single straight line can keep both 0s on one side and both 1s on the other.\n",
        "\n",
        "**Q24. What change is required to solve XOR successfully?**\n",
        "We must use a **Multi-Layer Perceptron (MLP)** by adding at least one Hidden Layer with a non-linear activation function.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“ Text Cell 4: Summary\n",
        "\n",
        "**Q25. List two strengths of a perceptron.**\n",
        "\n",
        "1. Very simple and computationally fast to implement.\n",
        "2. Works perfectly for linearly separable datasets.\n",
        "\n",
        "**Q26. List two limitations of a perceptron.**\n",
        "\n",
        "1. Cannot solve non-linear problems (like XOR).\n",
        "2. Requires manual feature engineering to work well on complex data.\n",
        "\n",
        "**Q27. When should we avoid using a single-layer perceptron?**\n",
        "When dealing with complex, non-linear data like images or unstructured text. For instance, detecting plant diseases from leaf images requires deep networks with many layers (like ResNet50 or custom CNNs) to extract intricate visual patterns; a single perceptron would completely fail at this task.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ’» Code Cell 1: Mini Challenge (Very Easy)\n",
        "\n",
        "**Q28. Mini Challenge Code Solution**\n",
        "\n",
        "```python\n",
        "# Given values\n",
        "x1, x2 = 1, 1\n",
        "w1, w2 = 1, 1\n",
        "bias = -1.5\n",
        "\n",
        "# Step 1: Calculate the weighted sum\n",
        "z = (x1 * w1) + (x2 * w2) + bias\n",
        "print(f\"Weighted sum (z) is: {z}\")\n",
        "\n",
        "# Step 2: Apply Step Activation Function\n",
        "# If z >= 0, output is 1. Else, 0.\n",
        "output = 1 if z >= 0 else 0\n",
        "\n",
        "print(f\"The perceptron output is: {output}\")\n",
        "\n",
        "# Reasoning in one line:\n",
        "# The weighted sum is (1*1) + (1*1) - 1.5 = 0.5. Since 0.5 >= 0, the step function outputs 1.\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "WvY3KA_xpGkx"
      }
    }
  ]
}